{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-24 12:59:02.434069: W external/xla/xla/service/gpu/nvptx_compiler.cc:679] The NVIDIA driver's CUDA version is 12.0 which is older than the ptxas CUDA version (12.3.103). Because the driver is older than the ptxas version, XLA is disabling parallel compilation, which may slow down compilation. You should update your NVIDIA driver or use the NVIDIA-provided CUDA forward compatibility packages.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Dataset modules for loading HDF5 simulation trajectories.\"\"\"\n",
    "\n",
    "import bisect\n",
    "import json\n",
    "import os\n",
    "import os.path as osp\n",
    "import re\n",
    "import zipfile\n",
    "from typing import Callable, Optional\n",
    "\n",
    "import h5py\n",
    "import jax.numpy as jnp\n",
    "import numpy as np\n",
    "import wget\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "from lagrangebench.utils import NodeType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset_name_from_path(path: str) -> str:\n",
    "    \"\"\"Infer the dataset name from the provided path.\n",
    "\n",
    "    This function assumes that the dataset directory name has the following structure:\n",
    "    {2D|3D}_{TGV|RPF|LDC|DAM}_{num_particles_max}_{num_steps}every{sampling_rate}\n",
    "\n",
    "    The dataset name then becomes one of the following:\n",
    "    {tgv2d|tgv3d|rpf2d|rpf3d|ldc2d|ldc3d|dam2d}\n",
    "    \"\"\"\n",
    "\n",
    "    name = re.search(r\"(?:2D|3D)_[A-Z]{2}\", path)\n",
    "    assert name is not None, (\n",
    "        f\"No valid dataset name found in path {path}. \"\n",
    "        \"Valid name formats: {2D|3D}_{HT|TGV|RPF|LDC|DAM} \"\n",
    "        \"Alternatively, you can specify the dataset name explicitly.\"\n",
    "    )\n",
    "    name = name.group(0)\n",
    "    name = f\"{name.split('_')[1]}{name.split('_')[0]}\".lower()\n",
    "    return name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split: str = \"train\"\n",
    "dataset_path: str = \"datasets/2D_HT_2500_10kevery100\"\n",
    "#name: Optional[str] = None,\n",
    "\n",
    "nl_backend: str = \"jaxmd_vmap\",\n",
    "#external_force_fn: Optional[Callable] = None,\n",
    "\n",
    "type(dataset_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset_path = \"datasets/2D_HT_2500_10kevery100\"\n",
    "input_seq_length: int =6\n",
    "\n",
    "extra_seq_length: int =0\n",
    "\n",
    "#type(dataset_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'datasets/2D_HT_2500_10kevery100'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#dataset_path: str = \"datasets/2D_HT_2500_10kevery100\"\n",
    "if dataset_path.endswith(\"/\"):  # remove trailing slash in dataset path\n",
    "            dataset_path = dataset_path[:-1]\n",
    "\n",
    "dataset_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "name=None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ht2d'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if name is None:\n",
    "    name = get_dataset_name_from_path(dataset_path)\n",
    "\n",
    "else:\n",
    "    name = name\n",
    "\n",
    "name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data available\n"
     ]
    }
   ],
   "source": [
    "if not osp.exists(dataset_path):\n",
    "    print('no data')\n",
    "else:\n",
    "    print('data available')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert split in [\"train\", \"valid\", \"test\"]\n",
    "assert (\n",
    "            input_seq_length > 1\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'datasets/2D_HT_2500_10kevery100/train.h5'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_path = osp.join(dataset_path, split + \".h5\")\n",
    "file_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "nl_backend: str = \"jaxmd_vmap\"\n",
    "external_force_fn: Optional[Callable] = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(osp.join(dataset_path, \"metadata.json\"), \"r\") as f:\n",
    "            metadata = json.loads(f.read())\n",
    "\n",
    "db_hdf5 = None\n",
    "\n",
    "#metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "61\n",
      "['00000', '00001', '00002', '00003']\n"
     ]
    }
   ],
   "source": [
    "with h5py.File(file_path, \"r\") as f:\n",
    "    traj_keys = list(f.keys())\n",
    "    sequence_length = f[\"00000/position\"].shape[0]\n",
    "print(sequence_length)\n",
    "\n",
    "print(traj_keys)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'split' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43msplit\u001b[49m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m      2\u001b[0m             \u001b[38;5;66;03m# During training, the first input_seq_length steps can only be used as\u001b[39;00m\n\u001b[1;32m      3\u001b[0m             \u001b[38;5;66;03m# input, and the last one to compute the target acceleration. If we use\u001b[39;00m\n\u001b[1;32m      4\u001b[0m             \u001b[38;5;66;03m# pushforward, then we need to provide extra_seq_length more steps\u001b[39;00m\n\u001b[1;32m      5\u001b[0m             \u001b[38;5;66;03m# from the end of a trajectory. Thus, the number of training samples per\u001b[39;00m\n\u001b[1;32m      6\u001b[0m             \u001b[38;5;66;03m# trajectory becomes:\u001b[39;00m\n\u001b[1;32m      7\u001b[0m             subseq_length \u001b[38;5;241m=\u001b[39m input_seq_length \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m extra_seq_length\n\u001b[1;32m      8\u001b[0m             samples_per_traj \u001b[38;5;241m=\u001b[39m sequence_length \u001b[38;5;241m-\u001b[39m subseq_length \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'split' is not defined"
     ]
    }
   ],
   "source": [
    "if split == \"train\":\n",
    "            # During training, the first input_seq_length steps can only be used as\n",
    "            # input, and the last one to compute the target acceleration. If we use\n",
    "            # pushforward, then we need to provide extra_seq_length more steps\n",
    "            # from the end of a trajectory. Thus, the number of training samples per\n",
    "            # trajectory becomes:\n",
    "            subseq_length = input_seq_length + 1 + extra_seq_length\n",
    "            samples_per_traj = sequence_length - subseq_length + 1\n",
    "\n",
    "            keylens = jnp.array([samples_per_traj for _ in range(len(traj_keys))])\n",
    "            _keylen_cumulative = jnp.cumsum(keylens).tolist()\n",
    "            num_samples = sum(keylens)\n",
    "            getter = get_window()\n",
    "\n",
    "num_samples"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
